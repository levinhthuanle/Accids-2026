\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{anderson2018bottom}
Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,
  L.: Bottom-up and top-down attention for image captioning and visual question
  answering. In: Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR). pp. 6077--6086 (2018)

\bibitem{bochkovskiy2020yolov4}
Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: Yolov4: Optimal speed and accuracy
  of object detection. arXiv preprint arXiv:2004.10934  (2020)

\bibitem{YOLO_family_ref}
Cao, C., Song, Y., He, J.: An overview of yolo series: From yolov1 to yolov8.
  Journal of Physics: Conference Series  \textbf{2685}(1),  012002 (2023)

\bibitem{EntityLinkingRef}
Chen, Y., Ma, L., Huang, Y., Zhang, S., Qian, Y.: A survey of knowledge
  graph-based entity linking from text to knowledge graph. IEEE Access
  \textbf{9},  13000--13020 (2021)

\bibitem{cornia2020meshed}
Cornia, M., Stefanini, M., Baraldi, L., Cucchiara, R.: Meshed-memory
  transformer for image captioning. In: IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR). pp. 10578--10587 (2020)

\bibitem{gillies2007shapely}
Gillies, S.: Shapely: manipulation and analysis of planar geometric objects
  (2007), python package, available at \url{https://pypi.org/project/Shapely/}

\bibitem{karpathy2015deep}
Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image
  descriptions. In: Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR). pp. 3128--3137 (2015)

\bibitem{krishna2017visual}
Krishna, R., et~al.: Visual genome: Connecting language and vision using
  crowdsourced dense image annotations. International Journal of Computer
  Vision  \textbf{123}(1),  32--73 (2017)

\bibitem{li2019entangled}
Li, G., Zhu, L., Liu, P., Yang, Y.: Entangled transformer for image captioning.
  In: IEEE/CVF International Conference on Computer Vision (ICCV). pp.
  8928--8937 (2019)

\bibitem{marino2017ok}
Marino, K., Salakhutdinov, R., Gupta, A.: The more you know: Using knowledge
  graphs for image classification. In: IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR). pp. 2673--2681 (2017)

\bibitem{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a
  unified text-to-text transformer. Journal of Machine Learning Research
  \textbf{21}(140),  1--67 (2020)

\bibitem{redmon2016you}
Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once:
  Unified, real-time object detection. In: IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR). pp. 779--788 (2016)

\bibitem{vinyals2015show}
Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image
  caption generator. In: Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR). pp. 3156--3164 (2015)

\bibitem{YOLOv7_ref}
Wang, C.Y., Bochkovskiy, A., Liao, H.Y.M.: Yolov7: Trainable bag-of-freebies
  for real-time object detection. arXiv preprint arXiv:2207.02696  (2022)

\bibitem{KGforVQARef}
Wang, P., Wu, Q., Shen, C., van~den Hengel, A.: Fvqa: Fact-based visual
  question answering. IEEE Transactions on Pattern Analysis and Machine
  Intelligence (PAMI)  \textbf{40}(10),  2413--2427 (2018)

\bibitem{wu2017image}
Wu, Q., Shen, C., Wang, P., Dick, A., van~den Hengel, A.: Image captioning and
  visual question answering based on attributes and external knowledge. IEEE
  Transactions on Pattern Analysis and Machine Intelligence  \textbf{40}(6),
  1367--1381 (2017)

\bibitem{xu2015show}
Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel,
  R.S., Bengio, Y.: Show, attend and tell: Neural image caption generation with
  visual attention. In: Proceedings of the International Conference on Machine
  Learning (ICML). pp. 2048--2057 (2015)

\bibitem{yang2024depth}
Yang, L., Kang, B., Huang, Z., Zhao, Z., Xu, X., Feng, J., Zhao, H.: Depth
  anything v2. arXiv preprint arXiv:2406.09414  (2024)

\bibitem{yao2018exploring}
Yao, T., Pan, Y., Li, Y., Mei, T.: Exploring visual relationship for image
  captioning. In: European Conference on Computer Vision (ECCV). pp. 684--699
  (2018)

\bibitem{young2014image}
Young, P., Lai, A., Hodosh, M., Hockenmaier, J.: From image descriptions to
  visual denotations: New similarity metrics for semantic inference over event
  descriptions. Transactions of the Association for Computational Linguistics
  \textbf{2},  67--78 (2014)

\bibitem{SceneGraphRef}
Zellers, R., Yatskar, M., Thomson, S., Choi, Y.: Neural motifs: Scene graph
  generation with global context. In: IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR). pp. 5831--5840 (2018)

\bibitem{zhao2021knowledge}
Zhao, W., Wu, B., Ma, S.: Knowledge enhanced fine-grained image captioning. In:
  ACM International Conference on Multimedia (MM). pp. 1631--1640 (2021)

\bibitem{zhong2020comprehensive}
Zhong, Y., Wang, L., Chen, J., Yu, D., Li, Y.: Comprehensive image captioning
  via scene graph decomposition. In: European Conference on Computer Vision
  (ECCV). pp. 211--229 (2020)

\bibitem{KnowledgeBasedCaptioningRef}
Zhu, Y., Yang, Z., Salakhutdinov, R., Xing, E.P.: Incorporating commonsense
  knowledge into image captioning via graph convolutional networks. In:
  IEEE/CVF International Conference on Computer Vision (ICCV). pp. 9008--9017
  (2019)

\end{thebibliography}
