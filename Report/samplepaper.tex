% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage{cite}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%

\pagestyle{plain}
\begin{document}
%
\title{Apply KG to enhance Image Description Generation}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Le Vinh Thuan\inst{1} \and Nguyen Minh Khoa\inst{2} \and Nguyen Vinh Thanh\inst{3} \and Nguyen Thi Dinh \inst{4,}}

\authorrunning{L. V. Thuan, et al .}

\institute{%
$^{ \{1,2,3 \}}$Faculty of Information Technology, University of Science, VNU-HCM, \\ Ho Chi Minh City, Vietnam, \\
{$^4$Ho Chi Minh City University of Industry and Trade, Ho Chi Minh City, Vietnam} 
\\[6pt]
$^1$\email{lvthuan23@apcs.fitus.edu.vn}, 
$^2$\email{nmkhoa23@apcs.fitus.edu.vn}, 
$^3$\email{23120012@student.hcmus.edu.vn}, 
$^4$\email{dinhnt@huit.edu.vn}
}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in
150--250 words.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%

\section{Introduction}

The automatic generation of natural language descriptions from images has emerged as one of the most challenging and important tasks in computer vision and natural language processing \cite{vinyals2015show, anderson2018bottom}. This interdisciplinary field, commonly known as image captioning or image description, requires machines to not only recognize objects and their spatial relationships within images but also to generate coherent, contextually appropriate textual descriptions that capture the semantic essence of visual content \cite{xu2015show, anderson2018bottom}.

Recent advances in deep learning have significantly improved the performance of image description systems, with approaches ranging from encoder-decoder architectures \cite{vinyals2015show} to attention-based mechanisms \cite{xu2015show} and transformer-based models \cite{li2019entangled}. However, existing methods often struggle with generating rich, contextually aware descriptions that go beyond simple object enumeration and spatial relationships \cite{wang2019controllable, cornia2020meshed}.

Knowledge graphs have shown tremendous potential in enhancing various computer vision tasks by providing structured semantic information and enabling reasoning capabilities \cite{marino2017ok, wu2017image}. The integration of knowledge graphs with image understanding systems allows for more sophisticated semantic reasoning and can bridge the gap between visual perception and high-level conceptual understanding \cite{yao2018exploring, zhong2020comprehensive}. Furthermore, the incorporation of external knowledge bases enables systems to generate more informative and contextually rich descriptions by leveraging world knowledge beyond what is directly observable in the image \cite{zhao2021knowledge, li2022oscar}.

This paper presents a comprehensive four-stage pipeline for generating detailed image descriptions that combines state-of-the-art object detection with knowledge graph construction and natural language inference. Our approach employs YOLOv11 \cite{ultralytics2023yolov8} for robust object detection in the first stage, followed by knowledge graph construction and entity linking. Unlike traditional approaches that rely solely on visual features, our system incorporates enriched contextual data through knowledge graph reasoning to perform sophisticated contextual analysis and inference, ultimately generating more comprehensive and meaningful image descriptions.

The main contributions of this work are: (1) A novel four-stage architecture that seamlessly integrates computer vision, knowledge representation, and natural language processing for image description; (2) An enhanced knowledge graph-based reasoning approach that enriches visual understanding with external knowledge; (3) A streamlined contextual analysis module that focuses on enriched data interpretation; and (4) Comprehensive evaluation demonstrating the effectiveness of our approach in generating high-quality, contextually aware image descriptions.

\section{Related Work}

Research on semantically-rich Image Description Generation (IDG) intersects multiple key domains, including Computer Vision (CV), Knowledge Representation (specifically Knowledge Graphs, KG), and Natural Language Processing (NLP). Early studies in image description primarily focused on generating fluent but factually limited captions based on simple encoder-decoder architectures. More recent works have shifted towards incorporating explicit semantic structures to enrich the output. At the same time, the field of Knowledge Representation has been exploring effective methods to inject external, common-sense knowledge into visual tasks. This section reviews three main directions relevant to the proposed framework: Object Detection and Feature Extraction for IDG, Knowledge-Enhanced Semantic Grounding, and Contextual Inference and Natural Language Generation.

\subsection{Object Detection and Feature Extraction for IDG}

High-quality image description relies on robust detection and accurate attribute extraction from the visual input. Object detection frameworks, such as the YOLO family \cite{YOLO_ref} and Faster R-CNN \cite{FasterRCNN_ref}, have become the foundation for most modern vision-to-language models. Recent advancements in object detection, including variants that enhance performance on small objects or crowded scenes \cite{YOLOv7_ref, DETR_ref}, continuously provide more granular and reliable entity bounding boxes.

While most detection-based IDG models use bounding boxes to apply attention mechanisms \cite{Xu2015} or build Scene Graphs \cite{SceneGraphRef}, they often stop at basic object classification (e.g., "person," "car"). This leaves a gap in research tailored to adapting the raw visual output for explicit knowledge retrieval. Our approach specifically leverages the visual features extracted (e.g., object class, bounding box, detected attributes) as input for a subsequent semantic grounding phase, using a robust detector like YOLOv11 \cite{YOLO_family_ref} to ensure high precision in entity localization.

\subsection{Knowledge-Enhanced Semantic Grounding}

To move beyond superficial descriptions, several studies have focused on incorporating external knowledge. Knowledge Graphs (KGs) are the most widely adopted form of structured external knowledge, proving vital in providing common-sense or domain-specific facts \cite{KGforVQARef}.

The core challenge is Semantic Grounding: effectively mapping coarse visual detections to specific KG entities. Techniques like Entity Linking \cite{EntityLinkingRef} are used to resolve ambiguities (e.g., distinguishing "apple" as a fruit vs. a company). Some models apply KG features during the decoding phase, primarily to aid vocabulary selection or fact-checking \cite{KnowledgeBasedCaptioningRef}. However, this late integration often fails to fundamentally change the input structure of the generation model. Our methodology differentiates by placing the KG at the center of the pipeline, using it to systematically enrich all raw visual entities and attributes with specific semantic details before the generation phase, thereby transforming a simple object list into a dense semantic structure.

\subsection{Contextual Inference and Natural Language Generation (NLG)}

The final stage of IDG involves converting the enriched semantic data into coherent, natural sentences. Conventional methods often rely on powerful sequence-to-sequence models to implicitly learn the mapping from features to text \cite{Vinyals2015}. However, when dealing with highly structured, fact-dense input (like the output of a KG), implicit mapping can lead to factual omissions or illogical sentence structure.

Early Template-based models \cite{TemplateBasedRef} offered logical structure but lacked fluency. More sophisticated methods employ explicit Relational Inference to discover implicit actions or states between objects \cite{RelationalInferenceRef}. Our work emphasizes a dedicated inference layer that operates on the structured, KG-enriched data. This layer's role is to perform Contextual Analysis and Logical Structuringâ€”essentially prioritizing the semantic facts and organizing them into a logical flow. This is crucial for satisfying the goal of generating a detailed "description" (which requires a narrative flow) rather than a simple "caption" (a single descriptive sentence). The structured output then feeds into a robust NLG module (potentially template-based or an advanced decoder) to ensure both high fluency and factual accuracy.

\subsection{Research Gap and Contribution}

Previous research highlights the complementary strengths of advanced object detectors and external knowledge sources. However, most methods remain limited either by their reliance on implicit reasoning within the decoder or by a superficial integration of knowledge (e.g., only using it for fact-checking). A unified framework that systematically leverages structured knowledge for entity enrichment and uses this enriched data for explicit, structured inference before NLG remains a significant challenge. Addressing this gap requires a unified framework that tightly couples robust visual analysis with a structured KG and a dedicated inference mechanism to deliver accurate, scalable, and semantically deep Image Description Generation.

\section{Theoretical Basis}

This section presents the theoretical foundations underlying our four-stage architecture, providing formal definitions and mathematical models that support the integration of computer vision, knowledge graph reasoning, and natural language generation.

\subsection{Object Detection and Spatial Analysis Framework}

\subsubsection{YOLOv11 Detection Model}
The YOLOv11 object detection framework can be formalized as a function $f_{YOLO}: \mathbb{R}^{H \times W \times 3} \rightarrow \mathcal{D}$, where the input image $I \in \mathbb{R}^{H \times W \times 3}$ is mapped to a detection set $\mathcal{D} = \{d_1, d_2, ..., d_n\}$. Each detection $d_i$ is represented as:

\begin{equation}
d_i = (bbox_i, c_i, s_i)
\end{equation}

where $bbox_i = (x_i, y_i, w_i, h_i)$ represents the bounding box coordinates, $c_i \in \mathcal{C}$ is the class label from the predefined class set $\mathcal{C}$, and $s_i \in [0,1]$ is the confidence score.

\subsubsection{Depth Estimation and Spatial Reasoning}
We incorporate the Depth-Anything-V2 model \cite{yang2024depth} to estimate depth information, formally defined as:

\begin{equation}
f_{depth}: \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H \times W}
\end{equation}

The depth map $D = f_{depth}(I)$ provides spatial context that enhances object relationship understanding. Combined with the Shapely geometry library \cite{gillies2007shapely}, we compute spatial relationships between detected objects using geometric operations:

\begin{equation}
R_{spatial}(d_i, d_j) = \{distance(d_i, d_j), overlap(d_i, d_j), relative\_position(d_i, d_j)\}
\end{equation}

\subsection{Knowledge Graph Construction Theory}

\subsubsection{Entity Linking and Knowledge Fusion}
Given the detection set $\mathcal{D}$ from Stage I, we define the entity linking function as:

\begin{equation}
f_{link}: \mathcal{D} \times \mathcal{KB} \rightarrow \mathcal{E}_{linked}
\end{equation}

where $\mathcal{KB}$ represents the external knowledge base and $\mathcal{E}_{linked}$ is the set of linked entities. The linking process employs semantic similarity scoring:

\begin{equation}
sim(d_i, e_k) = \alpha \cdot sim_{text}(c_i, label(e_k)) + \beta \cdot sim_{context}(context(d_i), context(e_k))
\end{equation}

where $\alpha$ and $\beta$ are weighting parameters, and $sim_{text}$ and $sim_{context}$ represent textual and contextual similarity measures respectively.

\subsubsection{Knowledge Graph Formalization}
The constructed knowledge graph is formally defined as a directed graph $KG = (V, E, R, A)$ where:
\begin{itemize}
    \item $V = \{v_1, v_2, ..., v_m\}$ is the set of entity vertices
    \item $E \subseteq V \times V$ represents the edges between entities
    \item $R$ is the set of relation types
    \item $A: V \rightarrow \mathcal{P}(\mathcal{A})$ maps entities to their attribute sets
\end{itemize}

\subsection{T5-based Natural Language Generation Framework}

\subsubsection{Text-to-Text Transfer Learning}
Our approach leverages the T5 (Text-to-Text Transfer Transformer) model \cite{raffel2020exploring} for generating natural language descriptions from structured semantic representations. The T5 framework treats all NLP tasks as text-to-text problems:

\begin{equation}
f_{T5}: \mathcal{S}_{semantic} \rightarrow \mathcal{T}_{text}
\end{equation}

where $\mathcal{S}_{semantic}$ represents the semantic input derived from the knowledge graph and $\mathcal{T}_{text}$ is the generated natural language output.

\subsubsection{Semantic Representation Encoding}
The knowledge graph entities and relationships are encoded into a structured semantic representation $S_{semantic}$ that serves as input to the T5 model:

\begin{equation}
S_{semantic} = encode(KG, R_{spatial}, C_{context})
\end{equation}

where $C_{context}$ represents the contextual information derived from the enriched data analysis.

\subsubsection{Attention Mechanism for Semantic Focus}
The T5 model employs multi-head self-attention to focus on relevant semantic elements:

\begin{equation}
Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $Q$, $K$, and $V$ represent query, key, and value matrices respectively, derived from the semantic representation.

\subsection{Contextual Reasoning and Inference Theory}

\subsubsection{Enriched Data Analysis}
Our approach performs contextual reasoning using enriched data from the knowledge graph without explicit relationship modeling. The contextual inference function is defined as:

\begin{equation}
f_{inference}: \mathcal{E}_{enriched} \rightarrow \mathcal{I}_{context}
\end{equation}

where $\mathcal{E}_{enriched}$ represents the enriched entity set and $\mathcal{I}_{context}$ denotes the inferred contextual information.

\subsubsection{Semantic Coherence Optimization}
To ensure semantic coherence in the generated descriptions, we optimize the following objective function:

\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{generation} + \lambda_1 \mathcal{L}_{coherence} + \lambda_2 \mathcal{L}_{semantic}
\end{equation}

where $\mathcal{L}_{generation}$ is the standard language modeling loss, $\mathcal{L}_{coherence}$ measures textual coherence, $\mathcal{L}_{semantic}$ ensures semantic consistency with the input knowledge graph, and $\lambda_1$, $\lambda_2$ are regularization parameters.

\subsection{Multi-Modal Integration Theory}

\subsubsection{Cross-Modal Alignment}
The integration of visual features, spatial information, and semantic knowledge requires cross-modal alignment. We define the alignment function as:

\begin{equation}
f_{align}: \mathcal{F}_{visual} \times \mathcal{F}_{spatial} \times \mathcal{F}_{semantic} \rightarrow \mathcal{F}_{unified}
\end{equation}

where $\mathcal{F}_{visual}$, $\mathcal{F}_{spatial}$, and $\mathcal{F}_{semantic}$ represent visual, spatial, and semantic feature spaces respectively, and $\mathcal{F}_{unified}$ is the unified representation space.

\subsubsection{Information Fusion Strategy}
The multi-modal information fusion employs weighted combination of feature representations:

\begin{equation}
\mathcal{F}_{unified} = \sum_{i=1}^{3} w_i \cdot \phi_i(\mathcal{F}_i)
\end{equation}

where $\phi_i$ represents the transformation function for each modality, and $w_i$ are learned weights that adapt to the relative importance of each information source.

This theoretical framework provides the mathematical foundation for our four-stage architecture, ensuring principled integration of computer vision, knowledge representation, and natural language generation components.

\section{Proposed Approach}

This section presents our comprehensive four-stage architecture for knowledge graph-enhanced image description generation. Our approach integrates advanced computer vision techniques with knowledge representation and natural language processing to generate rich, contextually aware image descriptions.

\subsection{Overall Architecture}

Figure \ref{fig:architecture} illustrates the complete pipeline of our proposed system. The architecture consists of four interconnected stages: (1) Computer Vision Processing using YOLOv11 and depth estimation, (2) Knowledge Graph Construction with entity linking, (3) Contextual Analysis and Inference using enriched data, and (4) Natural Language Generation powered by T5 transformer. Each stage contributes essential components that collectively enable sophisticated image understanding and description generation.

\subsection{Stage I: Enhanced Computer Vision Processing}

The first stage performs comprehensive visual analysis using multiple computer vision techniques to extract rich visual information from input images.

\subsubsection{Multi-Scale Object Detection}
Our system employs YOLOv11 \cite{ultralytics2023yolov8} for robust object detection, which provides:
\begin{itemize}
    \item Real-time object detection with high accuracy
    \item Multi-scale feature extraction for objects of varying sizes
    \item Confidence-based filtering to ensure reliable detections
\end{itemize}

The detection process generates a comprehensive object set $O = \{o_1, o_2, ..., o_n\}$, where each object $o_i$ contains class information, spatial coordinates, and confidence scores.

\subsubsection{Depth-Based Spatial Understanding}
We integrate Depth-Anything-V2-Base \cite{yang2024depth} to obtain detailed depth information, enabling:
\begin{itemize}
    \item 3D spatial relationship understanding between objects
    \item Distance estimation for relative positioning
    \item Depth-aware scene composition analysis
\end{itemize}

The depth estimation provides spatial context that enhances the understanding of object interactions and scene layout.

\subsubsection{Geometric Relationship Analysis}
Using the Shapely geometry library \cite{gillies2007shapely}, we compute precise geometric relationships:
\begin{itemize}
    \item Spatial overlaps and intersections between object regions
    \item Relative positioning (above, below, left, right, inside, outside)
    \item Distance calculations for proximity analysis
    \item Containment relationships for hierarchical object understanding
\end{itemize}

\subsubsection{Optical Character Recognition}
For images containing textual elements, we incorporate OCR capabilities to:
\begin{itemize}
    \item Extract textual information from signs, labels, and documents
    \item Integrate text as additional contextual entities
    \item Enhance semantic understanding through textual cues
\end{itemize}

\subsection{Stage II: Knowledge Graph Construction and Entity Linking}

The second stage transforms visual detections into a structured knowledge representation that enables semantic reasoning and contextual understanding.

\subsubsection{Entity Enrichment and Aggregation}
Detected objects are enhanced with additional semantic information:
\begin{itemize}
    \item Visual attributes (color, size, texture, orientation)
    \item Spatial properties derived from depth analysis
    \item Contextual tags based on scene understanding
    \item Confidence-weighted importance scoring
\end{itemize}

\subsubsection{External Knowledge Base Integration}
Our system performs entity linking with multiple knowledge sources:
\begin{itemize}
    \item \textbf{ConceptNet}: For common-sense relationships and properties
    \item \textbf{WordNet}: For semantic hierarchies and synonyms
    \item \textbf{YAGO/DBpedia}: For factual information and entity properties
    \item \textbf{Visual Genome}: For visual relationship patterns
\end{itemize}

The linking process employs semantic similarity matching:
\begin{equation}
similarity(e_{visual}, e_{kb}) = \alpha \cdot sim_{text}(label(e_{visual}), label(e_{kb})) + \beta \cdot sim_{context}(context(e_{visual}), context(e_{kb}))
\end{equation}

\subsubsection{Dynamic Knowledge Graph Construction}
The system constructs a scene-specific knowledge graph $KG = (V, E, R, A)$ where:
\begin{itemize}
    \item $V$ contains both detected visual entities and linked knowledge entities
    \item $E$ represents relationships derived from spatial analysis and knowledge linking
    \item $R$ includes spatial, semantic, and functional relationship types
    \item $A$ maps entities to their enriched attribute sets
\end{itemize}

\subsection{Stage III: Contextual Analysis and Enriched Data Processing}

The third stage focuses on sophisticated reasoning using the enriched knowledge graph without explicit relationship modeling.

\subsubsection{Semantic Enrichment Strategy}
Unlike traditional approaches that explicitly model all relationships, our system focuses on enriching entities with contextual information:
\begin{itemize}
    \item \textbf{Semantic Context}: Inferring implicit meanings from entity combinations
    \item \textbf{Functional Context}: Understanding purposes and activities
    \item \textbf{Temporal Context}: Inferring time-related aspects from visual cues
    \item \textbf{Causal Context}: Identifying cause-effect relationships
\end{itemize}

\subsubsection{Multi-Level Inference Engine}
Our inference engine operates at multiple abstraction levels:

\begin{enumerate}
    \item \textbf{Object-Level Inference}: Direct properties and attributes
    \item \textbf{Scene-Level Inference}: Overall scene understanding and context
    \item \textbf{Activity-Level Inference}: Actions and events happening in the scene
    \item \textbf{Conceptual-Level Inference}: High-level concepts and themes
\end{enumerate}

\subsubsection{Context Propagation Mechanism}
The system employs a context propagation algorithm that:
\begin{itemize}
    \item Spreads semantic information through the knowledge graph
    \item Weights context based on spatial proximity and semantic similarity
    \item Resolves ambiguities through multi-source evidence combination
    \item Maintains uncertainty estimates for probabilistic reasoning
\end{itemize}

\subsection{Stage IV: T5-Based Natural Language Generation}

The final stage employs the T5 (Text-to-Text Transfer Transformer) model \cite{raffel2020exploring} to generate natural language descriptions from the enriched semantic representation.

\subsubsection{Semantic-to-Text Encoding}
The enriched knowledge graph is converted into a structured semantic representation:
\begin{itemize}
    \item Entity-centric encoding highlighting important objects and their properties
    \item Relationship-aware structuring preserving spatial and semantic connections
    \item Context-enriched formatting including inferred information
    \item Hierarchical organization from concrete objects to abstract concepts
\end{itemize}

\subsubsection{T5 Model Adaptation}
We fine-tune the T5 model specifically for our image description task:
\begin{itemize}
    \item \textbf{Input Format}: Structured semantic representations derived from knowledge graphs
    \item \textbf{Output Format}: Natural language descriptions with varying levels of detail
    \item \textbf{Training Strategy}: Multi-task learning with description generation and semantic consistency
    \item \textbf{Attention Mechanism}: Enhanced attention over semantic structures
\end{itemize}

\subsubsection{Description Generation Logic}
The text generation process follows a structured approach:

\begin{enumerate}
    \item \textbf{Content Planning}: Organizing semantic information into narrative structure
    \item \textbf{Sentence Structure Logic}: Constructing grammatically correct and coherent sentences
    \item \textbf{Style Adaptation}: Adjusting linguistic style based on content type and context
    \item \textbf{Coherence Optimization}: Ensuring logical flow and narrative consistency
\end{enumerate}

\subsubsection{Multi-Level Description Generation}
Our system generates descriptions at multiple levels of detail:
\begin{itemize}
    \item \textbf{Basic Level}: Simple object enumeration and spatial relationships
    \item \textbf{Detailed Level}: Rich descriptions including attributes, activities, and context
    \item \textbf{Narrative Level}: Story-like descriptions with inferred activities and emotions
    \item \textbf{Technical Level}: Precise descriptions suitable for accessibility applications
\end{itemize}

\subsection{Integration and Optimization}

\subsubsection{End-to-End Training Strategy}
While individual components are pre-trained separately, the system employs end-to-end fine-tuning:
\begin{itemize}
    \item Joint optimization of knowledge graph construction and text generation
    \item Reinforcement learning for description quality improvement
    \item Multi-objective optimization balancing accuracy, fluency, and informativeness
\end{itemize}

\subsubsection{Quality Assurance Mechanisms}
The system incorporates several quality control measures:
\begin{itemize}
    \item Semantic consistency checking between visual content and generated text
    \item Factual accuracy verification against knowledge bases
    \item Linguistic quality assessment using automated metrics
    \item Diversity promotion to avoid repetitive descriptions
\end{itemize}

This comprehensive approach ensures that our system generates high-quality, contextually rich, and semantically accurate image descriptions that surpass traditional methods in both information content and linguistic quality.

\section{Evaluation}
\subsection{Environment and Experiment Data}

\section{Conclusion and Future Work}

\section*{Acknowledgments}
The authors would like to thank the Faculty of Information Technology, Ho Chi Minh
 City University of Industry and Trade, and University of Science, VNU-HCM, which
 are sponsors of this research. We also thank anonymous reviewers for their helpful
 comments on this paper.



% \section{First Section}


% \subsection{A Subsection Sample}
% Please note that the first paragraph of a section or subsection is
% not indented. The first paragraph that follows a table, figure,
% equation etc. does not need an indent, either.

% Subsequent paragraphs, however, are indented.

% \subsubsection{Sample Heading (Third Level)} Only two levels of
% headings should be numbered. Lower level headings remain unnumbered;
% they are formatted as run-in headings.

% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).

% \begin{figure}
% \includegraphics[width=\textwidth]{fig1.eps}
% \caption{A figure caption is always placed below the illustration.
% Please note that short captions are centered, while long ones are
% justified by the macro package automatically.} \label{fig1}
% \end{figure}

% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{Vinyals2015}, an LNCS chapter~\cite{SceneGraphRef}, a
% book~\cite{EntityLinkingRef}, proceedings without editors~\cite{TemplateRetrievalRef},
% and a homepage~\cite{KnowledgeBasedCaptioningRef}. Multiple citations are grouped
% \cite{Vinyals2015,SceneGraphRef,EntityLinkingRef},
% \cite{Vinyals2015,EntityLinkingRef,TemplateRetrievalRef,KnowledgeBasedCaptioningRef}.

% \begin{credits}
% \subsubsection{\ackname} A bold run-in heading in small font size at the end of the paper is
% used for general acknowledgments, for example: This study was funded
% by X (grant number Y).

% \subsubsection{\discintname}
% It is now necessary to declare any competing interests or to specifically
% state that the authors have no competing interests. Please place the
% statement with a bold run-in heading in small font size beneath the
% (optional) acknowledgments\footnote{If EquinOCS, our proceedings submission
% system, is used, then the disclaimer can be provided directly in the system.},
% for example: The authors have no competing interests to declare that are
% relevant to the content of this article. Or: Author A has received research
% grants from Company W. Author B has received a speaker honorarium from
% Company X and owns stock in Company Y. Author C is a member of committee Z.
% \end{credits}
% %
% % ---- Bibliography ----
% %
% % BibTeX users should specify bibliography style 'splncs04'.
% % References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{ref}
%
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}, last accessed 2023/10/25
% \end{thebibliography}



\end{document}
