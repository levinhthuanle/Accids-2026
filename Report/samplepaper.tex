% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage{cite}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%

\pagestyle{plain}
\begin{document}
%
\title{Spatio-Structural Image Captioning via LLM Fine-Tuning with Depth-Enhanced Scene Graphs}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Le Vinh Thuan\inst{1} \and Nguyen Minh Khoa\inst{2} \and Nguyen Vinh Thanh\inst{3} \and Nguyen Thi Dinh \inst{4,}}

\authorrunning{L. V. Thuan, et al .}

\institute{%
$^{ \{1,2,3 \}}$Faculty of Information Technology, University of Science, VNU-HCM, \\ Ho Chi Minh City, Vietnam, \\
{$^4$Ho Chi Minh City University of Industry and Trade, Ho Chi Minh City, Vietnam} 
\\[6pt]
$^1$\email{lvthuan23@apcs.fitus.edu.vn}, 
$^2$\email{nmkhoa23@apcs.fitus.edu.vn}, 
$^3$\email{23120012@student.hcmus.edu.vn}, 
$^4$\email{dinhnt@huit.edu.vn}
}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in
150--250 words.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%

\section{Introduction}

The automatic generation of natural language descriptions from images remains one of the most challenging and important tasks in computer vision and natural language processing \cite{vinyals2015show, anderson2018bottom}. This interdisciplinary field, commonly known as image captioning, requires machines to not only perceive objects and their relationships but also to generate coherent, contextually appropriate text that captures the scene's semantic essence \cite{xu2015show, anderson2018bottom}.
Recent advances in deep learning, particularly the adoption of Transformer-based models and Large Language Models (LLMs) in Vision-Language Pre-training (VLP) \cite{li2022blip, alayrac2022flamingo}, have significantly improved descriptive fluency. However, a critical limitation persists: current VLP and image captioning methods primarily rely on 2D feature extraction and lack a deep, grounded understanding of the 3D structure and spatial relationships within a scene. Consequently, generated captions often fail to accurately describe position, depth, and structural context (e.g., "the car is in front of the building" without specifying the distance or depth plane), limiting their applicability in domains requiring precise spatial reasoning, such as robotics or complex scene analysis.
To address this gap, Knowledge Graphs (KGs) offer a crucial pathway by providing structured semantic information and enabling reasoning capabilities \cite{marino2017ok, wu2017image}. While previous work has used KGs to incorporate general semantic and factual knowledge \cite{yao2018exploring, li2022oscar}, few have effectively integrated explicit structural and geometric knowledge like scene depth and 3D spatial relationships. Furthermore, existing KG-enhanced methods often lack commonsense reasoning, which is vital for generating captions that reflect human-like understanding of causality and purpose \cite{zhao2021knowledge}.
This paper introduces a novel framework, the Depth- and Commonsense-Augmented Scene Graph Captioner (DASG-CS Captioner), designed to generate fine-grained image descriptions with enhanced Spatio-Structural and Commonsense-Grounded awareness. We propose a comprehensive pipeline that moves beyond 2D perception by integrating state-of-the-art depth estimation into the knowledge representation stage, thereby enriching the generated captions with precise spatial details. Our method leverages the structured reasoning capabilities of Knowledge Graphs and the generative power of Fine-Tuned LLMs.
The main contributions of this work are:
A Novel Depth- and Commonsense-Augmented Scene Graph (DASG-CS): We propose a new KG architecture that seamlessly fuses visual features, quantitative depth information (from Depth Anything v2), and commonsense knowledge (from ConceptNet) to create a multi-faceted, structured representation of the scene.
Spatially-Aware LLM Fine-Tuning Strategy: We develop an effective encoding and fine-tuning strategy for LLMs, enabling the model to explicitly utilize the Spatio-Structural and Commonsense information encoded in the DASG-CS, leading to generated captions that are demonstrably richer in depth and contextual reasoning.
A Comprehensive Four-Stage Pipeline: We present a robust and reproducible pipeline that integrates feature extraction, multi-source knowledge fusion, structured data encoding, and natural language generation, setting a new benchmark for spatially and contextually grounded image captioning.
Extensive Evaluation with Novel Metrics: We conduct a comprehensive evaluation demonstrating the effectiveness of the DASG-CS framework, including specialized metrics to quantify the improvement in Spatial Reasoning Accuracy and Contextual Richness of the generated descriptions.
\section{Related Work}

Research on semantically-rich Image Description Generation (IDG) intersects multiple key domains, including Computer Vision (CV), Knowledge Representation (specifically Knowledge Graphs, KG), and Natural Language Processing (NLP). Early studies in image description primarily focused on generating fluent but factually limited captions based on simple encoder-decoder architectures. More recent works have shifted towards incorporating explicit semantic structures to enrich the output. At the same time, the field of Knowledge Representation has been exploring effective methods to inject external, common-sense knowledge into visual tasks. This section reviews three main directions relevant to the proposed framework: Object Detection and Feature Extraction for IDG, Knowledge-Enhanced Semantic Grounding, and Contextual Inference and Natural Language Generation.

\subsection{Object Detection and Feature Extraction for IDG}

High-quality image description relies on detection and accurate attribute extraction from the visual input. Object detection frameworks, such as the YOLO family \cite{YOLO_ref} and Faster R-CNN \cite{FasterRCNN_ref}, have become the foundation for most modern vision-to-language models. Recent advancements in object detection, including variants that enhance performance on small objects or crowded scenes \cite{YOLOv7_ref, DETR_ref}, continuously provide more granular and reliable entity bounding boxes.

While most detection-based IDG models use bounding boxes to apply attention mechanisms \cite{Xu2015} or build Scene Graphs \cite{SceneGraphRef}, they often stop at basic object classification (e.g., "person," "car"). This leaves a gap in research tailored to adapting the raw visual output for explicit knowledge retrieval. Our approach specifically leverages the visual features extracted (e.g., object class, bounding box, detected attributes) as input for a subsequent semantic grounding phase, using a detector like YOLOv11 \cite{YOLO_family_ref} to ensure high precision in entity localization.

\subsection{Knowledge-Enhanced Semantic Grounding}

To move beyond superficial descriptions, several studies have focused on incorporating external knowledge. Knowledge Graphs (KGs) are the most widely adopted form of structured external knowledge, proving vital in providing common-sense or domain-specific facts \cite{KGforVQARef}.

The core challenge is Semantic Grounding: effectively mapping coarse visual detections to specific KG entities. Techniques like Entity Linking \cite{EntityLinkingRef} are used to resolve ambiguities (e.g., distinguishing "apple" as a fruit vs. a company). Some models apply KG features during the decoding phase, primarily to aid vocabulary selection or fact-checking \cite{KnowledgeBasedCaptioningRef}. However, this late integration often fails to fundamentally change the input structure of the generation model. Our methodology differentiates by placing the KG at the center of the pipeline, using it to systematically enrich all raw visual entities and attributes with specific semantic details before the generation phase, thereby transforming a simple object list into a dense semantic structure.

\subsection{Contextual Inference and Natural Language Generation (NLG)}

The final stage of IDG involves converting the enriched semantic data into coherent, natural sentences. Conventional methods often rely on powerful sequence-to-sequence models to implicitly learn the mapping from features to text \cite{Vinyals2015}. However, when dealing with highly structured, fact-dense input (like the output of a KG), implicit mapping can lead to factual omissions or illogical sentence structure.

Early Template-based models \cite{TemplateBasedRef} offered logical structure but lacked fluency. More sophisticated methods employ explicit Relational Inference to discover implicit actions or states between objects \cite{RelationalInferenceRef}. Our work emphasizes a dedicated inference layer that operates on the structured, KG-enriched data. This layer's role is to perform Contextual Analysis and Logical Structuringâ€”essentially prioritizing the semantic facts and organizing them into a logical flow. This is crucial for satisfying the goal of generating a detailed "description" (which requires a narrative flow) rather than a simple "caption" (a single descriptive sentence). The structured output then feeds into a NLG module (potentially template-based or an advanced decoder) to ensure both high fluency and factual accuracy.

\subsection{Research Gap and Contribution}

Previous research highlights the complementary strengths of advanced object detectors and external knowledge sources. However, most methods remain limited either by their reliance on implicit reasoning within the decoder or by a superficial integration of knowledge (e.g., only using it for fact-checking). A unified framework that systematically leverages structured knowledge for entity enrichment and uses this enriched data for explicit, structured inference before NLG remains a significant challenge. Addressing this gap requires a unified framework that tightly couples visual analysis with a structured KG and a dedicated inference mechanism to deliver accurate, scalable, and semantically deep Image Description Generation.

\section{Theoretical Basis}

This section presents the theoretical foundations underlying our four-stage architecture, providing formal definitions and mathematical models that support the integration of computer vision, knowledge graph reasoning, and natural language generation.

\subsection{Object Detection and Spatial Analysis Framework}

\subsubsection{YOLOv11 Detection Model}
The YOLOv11 object detection framework can be formalized as a function $f_{YOLO}: \mathbb{R}^{H \times W \times 3} \rightarrow \mathcal{D}$, where the input image $I \in \mathbb{R}^{H \times W \times 3}$ is mapped to a detection set $\mathcal{D} = \{d_1, d_2, ..., d_n\}$. Each detection $d_i$ is represented as:

\begin{equation}
d_i = (bbox_i, c_i, s_i)
\end{equation}

where $bbox_i = (x_i, y_i, w_i, h_i)$ represents the bounding box coordinates, $c_i \in \mathcal{C}$ is the class label from the predefined class set $\mathcal{C}$, and $s_i \in [0,1]$ is the confidence score.

\subsubsection{Depth Estimation and Spatial Reasoning}
We incorporate the Depth-Anything-V2 model \cite{yang2024depth} to estimate depth information, formally defined as:

\begin{equation}
f_{depth}: \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H \times W}
\end{equation}

The depth map $D = f_{depth}(I)$ provides spatial context that enhances object relationship understanding. Combined with the Shapely geometry library \cite{gillies2007shapely}, we compute spatial relationships between detected objects using geometric operations:

\begin{equation}
R_{spatial}(d_i, d_j) = \{distance(d_i, d_j), overlap(d_i, d_j), relative\_position(d_i, d_j)\}
\end{equation}

\subsection{Knowledge Graph Construction Theory}

\subsubsection{Entity Linking and Knowledge Fusion}
Given the detection set $\mathcal{D}$ from Stage I, we define the entity linking function as:

\begin{equation}
f_{link}: \mathcal{D} \times \mathcal{KB} \rightarrow \mathcal{E}_{linked}
\end{equation}

where $\mathcal{KB}$ represents the external knowledge base and $\mathcal{E}_{linked}$ is the set of linked entities. The linking process employs semantic similarity scoring:

\begin{equation}
sim(d_i, e_k) = \alpha \cdot sim_{text}(c_i, label(e_k)) + \beta \cdot sim_{context}(context(d_i), context(e_k))
\end{equation}

where $\alpha$ and $\beta$ are weighting parameters, and $sim_{text}$ and $sim_{context}$ represent textual and contextual similarity measures respectively.

\subsubsection{Knowledge Graph Formalization}
The constructed knowledge graph is formally defined as a directed graph $KG = (V, E, R, A)$ where $V = \{v_1, v_2, ..., v_m\}$ is the set of entity vertices, $E \subseteq V \times V$ represents the edges between entities, $R$ is the set of relation types, and $A: V \rightarrow \mathcal{P}(\mathcal{A})$ maps entities to their attribute sets.

\subsection{T5-based Natural Language Generation Framework}

\subsubsection{Text-to-Text Transfer Learning}
Our approach leverages the T5 (Text-to-Text Transfer Transformer) model \cite{raffel2020exploring} for generating natural language descriptions from structured semantic representations. The T5 framework treats all NLP tasks as text-to-text problems:

\begin{equation}
f_{T5}: \mathcal{S}_{semantic} \rightarrow \mathcal{T}_{text}
\end{equation}

where $\mathcal{S}_{semantic}$ represents the semantic input derived from the knowledge graph and $\mathcal{T}_{text}$ is the generated natural language output.

\subsubsection{Semantic Representation Encoding}
The knowledge graph entities and relationships are encoded into a structured semantic representation $S_{semantic}$ that serves as input to the T5 model:

\begin{equation}
S_{semantic} = encode(KG, R_{spatial}, C_{context})
\end{equation}

where $C_{context}$ represents the contextual information derived from the enriched data analysis.

\subsubsection{Attention Mechanism for Semantic Focus}
The T5 model employs multi-head self-attention to focus on relevant semantic elements:

\begin{equation}
Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $Q$, $K$, and $V$ represent query, key, and value matrices respectively, derived from the semantic representation.

\subsection{Contextual Reasoning and Inference Theory}

\subsubsection{Enriched Data Analysis}
Our approach performs contextual reasoning using enriched data from the knowledge graph without explicit relationship modeling. The contextual inference function is defined as:

\begin{equation}
f_{inference}: \mathcal{E}_{enriched} \rightarrow \mathcal{I}_{context}
\end{equation}

where $\mathcal{E}_{enriched}$ represents the enriched entity set and $\mathcal{I}_{context}$ denotes the inferred contextual information.

\subsubsection{Semantic Coherence Optimization}
To ensure semantic coherence in the generated descriptions, we optimize the following objective function:

\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{generation} + \lambda_1 \mathcal{L}_{coherence} + \lambda_2 \mathcal{L}_{semantic}
\end{equation}

where $\mathcal{L}_{generation}$ is the standard language modeling loss, $\mathcal{L}_{coherence}$ measures textual coherence, $\mathcal{L}_{semantic}$ ensures semantic consistency with the input knowledge graph, and $\lambda_1$, $\lambda_2$ are regularization parameters.

\subsection{Multi-Modal Integration Theory}

\subsubsection{Cross-Modal Alignment}
The integration of visual features, spatial information, and semantic knowledge requires cross-modal alignment. We define the alignment function as:

\begin{equation}
f_{align}: \mathcal{F}_{visual} \times \mathcal{F}_{spatial} \times \mathcal{F}_{semantic} \rightarrow \mathcal{F}_{unified}
\end{equation}

where $\mathcal{F}_{visual}$, $\mathcal{F}_{spatial}$, and $\mathcal{F}_{semantic}$ represent visual, spatial, and semantic feature spaces respectively, and $\mathcal{F}_{unified}$ is the unified representation space.

\subsubsection{Information Fusion Strategy}
The multi-modal information fusion employs weighted combination of feature representations:

\begin{equation}
\mathcal{F}_{unified} = \sum_{i=1}^{3} w_i \cdot \phi_i(\mathcal{F}_i)
\end{equation}

where $\phi_i$ represents the transformation function for each modality, and $w_i$ are learned weights that adapt to the relative importance of each information source.

This theoretical framework provides the mathematical foundation for our four-stage architecture, ensuring principled integration of computer vision, knowledge representation, and natural language generation components.

\section{Proposed Approach}

This section presents our three-stage architecture for knowledge graph-enhanced image description generation. 
Our approach integrates advanced computer vision techniques with knowledge representation and natural language processing to generate rich, 
contextually aware image descriptions.

\subsection{Overall Architecture}

Figure \ref{fig:architecture} illustrates the complete pipeline of our proposed system. The architecture consists of three interconnected stages: (1) Computer Vision Processing using YOLOv11 and depth estimation, (2) Knowledge Graph Construction with entity linking, and (3) Contextual Analysis and Inference using enriched data, and (4) Natural Language Generation powered by T5 transformer. Each stage contributes essential components that collectively enable sophisticated image understanding and description generation.
\subsection{Stage I: Enhanced Computer Vision Processing}

The first stage performs comprehensive visual analysis using multiple computer vision techniques to extract rich visual information from input images.

\subsubsection{Multi-Scale Object Detection}
Our system employs YOLOv11 \cite{ultralytics2023yolov8} for  object detection, which provides real-time object detection with high accuracy, multi-scale feature extraction for objects of varying sizes, and confidence-based filtering to ensure reliable detections.

The detection process generates a comprehensive object set $O = \{o_1, o_2, ..., o_n\}$, where each object $o_i$ contains class information, spatial coordinates, and confidence scores.

\subsubsection{Depth-Based Spatial Understanding}
We integrate Depth-Anything-V2-Base \cite{yang2024depth} to obtain detailed depth information, enabling 3D spatial relationship understanding between objects, distance estimation for relative positioning, and depth-aware scene composition analysis.

The depth estimation provides spatial context that enhances the understanding of object interactions and scene layout.

\subsubsection{Geometric Relationship Analysis}
Using the Shapely geometry library \cite{gillies2007shapely}, we compute precise geometric relationships including spatial overlaps and intersections between object regions, relative positioning (above, below, left, right, inside, outside), distance calculations for proximity analysis, and containment relationships for hierarchical object understanding.

\subsubsection{Optical Character Recognition}
For images containing textual elements, we incorporate OCR capabilities to extract textual information from signs, labels, and documents, integrate text as additional contextual entities, and enhance semantic understanding through textual cues.

\subsection{Stage II: Knowledge Graph Construction and Entity Linking}

The second stage transforms visual detections into a structured knowledge representation that enables semantic reasoning and contextual understanding.

\subsubsection{Entity Enrichment and Aggregation}
Detected objects are enhanced with additional semantic information including visual attributes (color, size, texture, orientation), spatial properties derived from depth analysis, contextual tags based on scene understanding, and confidence-weighted importance scoring.

\subsubsection{External Knowledge Base Integration}
Our system performs entity linking with multiple knowledge sources: \textbf{ConceptNet} for common-sense relationships and properties, \textbf{WordNet} for semantic hierarchies and synonyms, \textbf{YAGO/DBpedia} for factual information and entity properties, and \textbf{Visual Genome} for visual relationship patterns.

The linking process employs semantic similarity matching:
\begin{equation}
similarity(e_{visual}, e_{kb}) = \alpha \cdot sim_{text}(label(e_{visual}), label(e_{kb})) + \beta \cdot sim_{context}(context(e_{visual}), context(e_{kb}))
\end{equation}

\subsubsection{Dynamic Knowledge Graph Construction}
The system constructs a scene-specific knowledge graph $KG = (V, E, R, A)$ where $V$ contains both detected visual entities and linked knowledge entities, $E$ represents relationships derived from spatial analysis and knowledge linking, $R$ includes spatial, semantic, and functional relationship types, and $A$ maps entities to their enriched attribute sets.

\subsection{Stage III: Contextual Analysis and Enriched Data Processing}

The third stage focuses on sophisticated reasoning using the enriched knowledge graph without explicit relationship modeling.

\subsubsection{Semantic Enrichment Strategy}
Unlike traditional approaches that explicitly model all relationships, our system focuses on enriching entities with contextual information. This includes \textbf{Semantic Context} for inferring implicit meanings from entity combinations, \textbf{Functional Context} for understanding purposes and activities, \textbf{Temporal Context} for inferring time-related aspects from visual cues, and \textbf{Causal Context} for identifying cause-effect relationships.

\subsubsection{Multi-Level Inference Engine}
Our inference engine operates at multiple abstraction levels: (1) \textbf{Object-Level Inference} for direct properties and attributes, (2) \textbf{Scene-Level Inference} for overall scene understanding and context, (3) \textbf{Activity-Level Inference} for actions and events happening in the scene, and (4) \textbf{Conceptual-Level Inference} for high-level concepts and themes.

\subsubsection{Context Propagation Mechanism}
The system employs a context propagation algorithm that spreads semantic information through the knowledge graph, weights context based on spatial proximity and semantic similarity, resolves ambiguities through multi-source evidence combination, and maintains uncertainty estimates for probabilistic reasoning.

\subsection{Stage IV: T5-Based Natural Language Generation}

The final stage employs the T5 (Text-to-Text Transfer Transformer) model \cite{raffel2020exploring} to generate natural language descriptions from the enriched semantic representation.

\subsubsection{Semantic-to-Text Encoding}
The enriched knowledge graph is converted into a structured semantic representation featuring entity-centric encoding highlighting important objects and their properties, relationship-aware structuring preserving spatial and semantic connections, context-enriched formatting including inferred information, and hierarchical organization from concrete objects to abstract concepts.

\subsubsection{T5 Model Adaptation}
We fine-tune the T5 model specifically for our image description task with \textbf{Input Format} as structured semantic representations derived from knowledge graphs, \textbf{Output Format} as natural language descriptions with varying levels of detail, \textbf{Training Strategy} using multi-task learning with description generation and semantic consistency, and \textbf{Attention Mechanism} enhanced attention over semantic structures.

\subsubsection{Description Generation Logic}
The text generation process follows a structured approach: (1) \textbf{Content Planning} for organizing semantic information into narrative structure, (2) \textbf{Sentence Structure Logic} for constructing grammatically correct and coherent sentences, (3) \textbf{Style Adaptation} for adjusting linguistic style based on content type and context, and (4) \textbf{Coherence Optimization} for ensuring logical flow and narrative consistency.

\subsubsection{Multi-Level Description Generation}
Our system generates descriptions at multiple levels of detail: \textbf{Basic Level} for simple object enumeration and spatial relationships, \textbf{Detailed Level} for rich descriptions including attributes, activities, and context, \textbf{Narrative Level} for story-like descriptions with inferred activities and emotions, and \textbf{Technical Level} for precise descriptions suitable for accessibility applications.

\subsection{Integration and Optimization}

\subsubsection{End-to-End Training Strategy}
While individual components are pre-trained separately, the system employs end-to-end fine-tuning through joint optimization of knowledge graph construction and text generation, reinforcement learning for description quality improvement, and multi-objective optimization balancing accuracy, fluency, and informativeness.

\subsubsection{Quality Assurance Mechanisms}
The system incorporates several quality control measures including semantic consistency checking between visual content and generated text, factual accuracy verification against knowledge bases, linguistic quality assessment using automated metrics, and diversity promotion to avoid repetitive descriptions.

This comprehensive approach ensures that our system generates high-quality, contextually rich, and semantically accurate image descriptions that surpass traditional methods in both information content and linguistic quality.

\section{Evaluation}
\subsection{Environment and Experiment Data}

\section{Future Work}



\section*{Acknowledgments}
The authors would like to thank the Faculty of Information Technology, Ho Chi Minh
 City University of Industry and Trade, and University of Science, VNU-HCM, which
 are sponsors of this research. We also thank anonymous reviewers for their helpful
 comments on this paper.

\bibliographystyle{splncs04}
\bibliography{ref}

\end{document}