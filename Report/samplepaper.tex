% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage{cite}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%

\pagestyle{plain}
\begin{document}
%
\title{Spatio-Structural Image Captioning via LLM Fine-Tuning with Depth-Enhanced Scene Graphs}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Le Vinh Thuan\inst{1} \and Nguyen Minh Khoa\inst{2} \and Nguyen Vinh Thanh\inst{3} \and Nguyen Thi Dinh \inst{4,}}

\authorrunning{L. V. Thuan, et al .}

\institute{%
$^{ \{1,2,3 \}}$Faculty of Information Technology, University of Science, VNU-HCM, \\ Ho Chi Minh City, Vietnam, \\
{$^4$Ho Chi Minh City University of Industry and Trade, Ho Chi Minh City, Vietnam} 
\\[6pt]
$^1$\email{lvthuan23@apcs.fitus.edu.vn}, 
$^2$\email{nmkhoa23@apcs.fitus.edu.vn}, 
$^3$\email{23120012@student.hcmus.edu.vn}, 
$^4$\email{dinhnt@huit.edu.vn}
}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in
150--250 words.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%

\section{Introduction}

The automatic generation of natural language descriptions from images remains one of the most challenging and important tasks in computer vision and natural language processing \cite{vinyals2015show, anderson2018bottom}. This interdisciplinary field, commonly known as image captioning, requires machines to not only perceive objects and their relationships but also to generate coherent, contextually appropriate text that captures the scene's semantic essence \cite{xu2015show, anderson2018bottom}.
Recent advances in deep learning, particularly the adoption of Transformer-based models and Large Language Models (LLMs) in Vision-Language Pre-training (VLP) \cite{li2022blip, alayrac2022flamingo}, have significantly improved descriptive fluency. However, a critical limitation persists: current VLP and image captioning methods primarily rely on 2D feature extraction and lack a deep, grounded understanding of the 3D structure and spatial relationships within a scene. Consequently, generated captions often fail to accurately describe position, depth, and structural context (e.g., "the car is in front of the building" without specifying the distance or depth plane), limiting their applicability in domains requiring precise spatial reasoning, such as robotics or complex scene analysis.
To address this gap, Knowledge Graphs (KGs) offer a crucial pathway by providing structured semantic information and enabling reasoning capabilities \cite{marino2017ok, wu2017image}. While previous work has used KGs to incorporate general semantic and factual knowledge \cite{yao2018exploring, li2022oscar}, few have effectively integrated explicit structural and geometric knowledge like scene depth and 3D spatial relationships. Furthermore, existing KG-enhanced methods often lack commonsense reasoning, which is vital for generating captions that reflect human-like understanding of causality and purpose \cite{zhao2021knowledge}.
This paper introduces a novel framework, the Depth- and Commonsense-Augmented Scene Graph Captioner (DASG-CS Captioner), designed to generate fine-grained image descriptions with enhanced Spatio-Structural and Commonsense-Grounded awareness. We propose a comprehensive pipeline that moves beyond 2D perception by integrating state-of-the-art depth estimation into the knowledge representation stage, thereby enriching the generated captions with precise spatial details. Our method leverages the structured reasoning capabilities of Knowledge Graphs and the generative power of Fine-Tuned LLMs.
The main contributions of this work are:
A Novel Depth- and Commonsense-Augmented Scene Graph (DASG-CS): We propose a new KG architecture that seamlessly fuses visual features, quantitative depth information (from Depth Anything v2), and commonsense knowledge (from ConceptNet) to create a multi-faceted, structured representation of the scene.
Spatially-Aware LLM Fine-Tuning Strategy: We develop an effective encoding and fine-tuning strategy for LLMs, enabling the model to explicitly utilize the Spatio-Structural and Commonsense information encoded in the DASG-CS, leading to generated captions that are demonstrably richer in depth and contextual reasoning.
A Three-Stage Pipeline: We present a pipeline that integrates feature extraction, multi-source knowledge fusion, structured data encoding, and natural language generation, setting a new benchmark for spatially and contextually grounded image captioning.
Extensive Evaluation with Novel Metrics: We conduct a comprehensive evaluation demonstrating the effectiveness of the DASG-CS framework, including specialized metrics to quantify the improvement in Spatial Reasoning Accuracy and Contextual Richness of the generated descriptions.
\section{Related Work}

Image captioning has evolved significantly from simple encoder-decoder architectures \cite{vinyals2015show} to sophisticated systems incorporating spatial awareness and external knowledge \cite{anderson2018bottom}. Our work builds upon three critical research directions: depth-aware image understanding, knowledge graph construction for vision-language tasks, and fine-tuning language models for spatial-aware captioning.

\subsection{Object Detection and Spatial Feature Extraction}

Traditional image captioning systems rely primarily on 2D visual features extracted from RGB images \cite{xu2015show, anderson2018bottom}. The YOLO family has evolved from YOLOv1's single-stage detection \cite{redmon2016you} through multiple generations \cite{bochkovskiy2020yolov4, YOLOv7_ref, YOLO_family_ref} to more sophisticated architectures. Modern object detectors like YOLOv11 provide real-time performance with improved accuracy, particularly for small objects and crowded scenes. While object detection has advanced significantly, most systems still focus on 2D bounding boxes without incorporating depth information for true 3D spatial understanding.

\subsection{Scene Understanding and Spatial Relationships}

Scene graph generation has emerged as a method to capture object relationships explicitly \cite{SceneGraphRef, yao2018exploring}. These approaches construct structured representations of visual scenes by identifying objects and their relationships \cite{zhong2020comprehensive}. However, most scene graphs focus on semantic relationships rather than precise metric spatial relationships. Our approach aims to incorporate depth estimation to enable more accurate spatial relationship modeling.

\subsection{Knowledge-Enhanced Image Captioning}

Knowledge graphs provide structured representations of visual scenes and external world knowledge, enabling richer semantic understanding. Visual Genome \cite{krishna2017visual} pioneered large-scale visual knowledge graphs with detailed annotations of objects, attributes, and relationships. Flickr30k \cite{young2014image} provide image-caption pairs widely used for training vision-language models.

Several works have explored incorporating external knowledge into image captioning. Knowledge graphs have been used for visual question answering \cite{KGforVQARef} and image classification \cite{marino2017ok}. Entity linking techniques \cite{EntityLinkingRef} help map visual detections to knowledge base entities. Knowledge-based approaches \cite{KnowledgeBasedCaptioningRef, zhao2021knowledge} have shown improvements by incorporating common-sense and factual knowledge.

Most existing approaches integrate knowledge at the generation stage \cite{wu2017image}. Our pipeline differs by constructing a comprehensive knowledge graph that fuses visual observations from datasets like Visual Genome and Flickr with external knowledge sources before the generation phase.

\subsection{Natural Language Generation for Image Captioning}

Natural language generation is the final component of image captioning systems. Early encoder-decoder models \cite{vinyals2015show, karpathy2015deep} used RNNs to generate captions from visual features. Attention mechanisms \cite{xu2015show, anderson2018bottom} improved performance by allowing models to focus on relevant image regions during generation.

More recent approaches employ transformer-based architectures \cite{li2019entangled, cornia2020meshed} for better long-range dependency modeling. The T5 model \cite{raffel2020exploring} treats all NLP tasks as text-to-text problems, offering a unified framework adaptable to various generation tasks including image captioning.

Our approach leverages text-to-text transformers for generating descriptions from structured semantic representations. By fine-tuning on data that explicitly encodes spatial relationships and depth information, we aim to generate descriptions that accurately reflect 3D spatial configurations.

\subsection{Research Gaps and Our Contributions}

Existing image captioning systems face several limitations:

\begin{enumerate}
\item \textbf{Limited Spatial Awareness:} Most systems \cite{vinyals2015show, xu2015show} rely on 2D visual features without depth information for true 3D spatial understanding.

\item \textbf{Knowledge Integration:} While some works integrate external knowledge \cite{marino2017ok, zhao2021knowledge}, comprehensive frameworks that systematically combine visual observations with external knowledge graphs remain limited.

\item \textbf{Spatial Description Quality:} Generated descriptions often lack precise spatial relationships and depth-aware positioning information.
\end{enumerate}

Our proposed three-system pipeline addresses these gaps:

\begin{enumerate}
\item \textbf{System 1 - Spatial Feature Extraction:} We combine YOLOv11 for object detection with Depth Anything V2 for monocular depth estimation, enabling extraction of objects, their relationships, and depth information.

\item \textbf{System 2 - Knowledge Graph Framework:} We construct knowledge graphs based on Visual Genome \cite{krishna2017visual} and Flickr \cite{young2014image} datasets, enriched with external knowledge from ConceptNet and DBpedia.

\item \textbf{System 3 - Depth-Aware Caption Generation:} We fine-tune language models to generate image descriptions with emphasis on spatial and depth features, producing captions that accurately reflect 3D spatial configurations.
\end{enumerate}

The key contribution is the systematic integration of depth information throughout the pipeline, from detection through knowledge graph construction to caption generation, emphasizing spatial accuracy in generated descriptions.



\section{Theoretical Basis}

This section presents the theoretical foundations underlying our three-stage architecture, providing formal definitions and mathematical models that support the integration of computer vision, knowledge graph reasoning, and natural language generation.

\subsection{Object Detection and Spatial Analysis Framework}

\subsubsection{YOLOv11 Detection Model}
The YOLOv11 object detection framework can be formalized as a function $f_{YOLO}: \mathbb{R}^{H \times W \times 3} \rightarrow \mathcal{D}$, where the input image $I \in \mathbb{R}^{H \times W \times 3}$ is mapped to a detection set $\mathcal{D} = \{d_1, d_2, ..., d_n\}$. Each detection $d_i$ is represented as:

\begin{equation}
d_i = (bbox_i, c_i, s_i)
\end{equation}

where $bbox_i = (x_i, y_i, w_i, h_i)$ represents the bounding box coordinates, $c_i \in \mathcal{C}$ is the class label from the predefined class set $\mathcal{C}$, and $s_i \in [0,1]$ is the confidence score.

\subsubsection{Depth Estimation and Spatial Reasoning}
We incorporate the Depth-Anything-V2 model \cite{yang2024depth} to estimate depth information, formally defined as:

\begin{equation}
f_{depth}: \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H \times W}
\end{equation}

The depth map $D = f_{depth}(I)$ provides spatial context that enhances object relationship understanding. Combined with the Shapely geometry library \cite{gillies2007shapely}, we compute spatial relationships between detected objects using geometric operations:

\begin{equation}
R_{spatial}(d_i, d_j) = \{distance(d_i, d_j), overlap(d_i, d_j), relative\_position(d_i, d_j)\}
\end{equation}

\subsection{Knowledge Graph Construction Theory}

\subsubsection{Entity Linking and Knowledge Fusion}
Given the detection set $\mathcal{D}$ from Stage I, we define the entity linking function as:

\begin{equation}
f_{link}: \mathcal{D} \times \mathcal{KB} \rightarrow \mathcal{E}_{linked}
\end{equation}

where $\mathcal{KB}$ represents the external knowledge base and $\mathcal{E}_{linked}$ is the set of linked entities. The linking process employs semantic similarity scoring:

\begin{equation}
sim(d_i, e_k) = \alpha \cdot sim_{text}(c_i, label(e_k)) + \beta \cdot sim_{context}(context(d_i), context(e_k))
\end{equation}

where $\alpha$ and $\beta$ are weighting parameters, and $sim_{text}$ and $sim_{context}$ represent textual and contextual similarity measures respectively.

\subsubsection{Knowledge Graph Formalization}
The constructed knowledge graph is formally defined as a directed graph $KG = (V, E, R, A)$ where $V = \{v_1, v_2, ..., v_m\}$ is the set of entity vertices, $E \subseteq V \times V$ represents the edges between entities, $R$ is the set of relation types, and $A: V \rightarrow \mathcal{P}(\mathcal{A})$ maps entities to their attribute sets.

\subsection{T5-based Natural Language Generation Framework}

\subsubsection{Text-to-Text Transfer Learning}
Our approach leverages the T5 (Text-to-Text Transfer Transformer) model \cite{raffel2020exploring} for generating natural language descriptions from structured semantic representations. The T5 framework treats all NLP tasks as text-to-text problems:

\begin{equation}
f_{T5}: \mathcal{S}_{semantic} \rightarrow \mathcal{T}_{text}
\end{equation}

where $\mathcal{S}_{semantic}$ represents the semantic input derived from the knowledge graph and $\mathcal{T}_{text}$ is the generated natural language output.

\subsubsection{Semantic Representation Encoding}
The knowledge graph entities and relationships are encoded into a structured semantic representation $S_{semantic}$ that serves as input to the T5 model:

\begin{equation}
S_{semantic} = encode(KG, R_{spatial}, C_{context})
\end{equation}

where $C_{context}$ represents the contextual information derived from the enriched data analysis.

\subsubsection{Attention Mechanism for Semantic Focus}
The T5 model employs multi-head self-attention to focus on relevant semantic elements:

\begin{equation}
Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $Q$, $K$, and $V$ represent query, key, and value matrices respectively, derived from the semantic representation.

\subsection{Contextual Reasoning and Inference Theory}

\subsubsection{Enriched Data Analysis}
Our approach performs contextual reasoning using enriched data from the knowledge graph without explicit relationship modeling. The contextual inference function is defined as:

\begin{equation}
f_{inference}: \mathcal{E}_{enriched} \rightarrow \mathcal{I}_{context}
\end{equation}

where $\mathcal{E}_{enriched}$ represents the enriched entity set and $\mathcal{I}_{context}$ denotes the inferred contextual information.

\subsubsection{Semantic Coherence Optimization}
To ensure semantic coherence in the generated descriptions, we optimize the following objective function:

\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{generation} + \lambda_1 \mathcal{L}_{coherence} + \lambda_2 \mathcal{L}_{semantic}
\end{equation}

where $\mathcal{L}_{generation}$ is the standard language modeling loss, $\mathcal{L}_{coherence}$ measures textual coherence, $\mathcal{L}_{semantic}$ ensures semantic consistency with the input knowledge graph, and $\lambda_1$, $\lambda_2$ are regularization parameters.

\subsection{Multi-Modal Integration Theory}

\subsubsection{Cross-Modal Alignment}
The integration of visual features, spatial information, and semantic knowledge requires cross-modal alignment. We define the alignment function as:

\begin{equation}
f_{align}: \mathcal{F}_{visual} \times \mathcal{F}_{spatial} \times \mathcal{F}_{semantic} \rightarrow \mathcal{F}_{unified}
\end{equation}

where $\mathcal{F}_{visual}$, $\mathcal{F}_{spatial}$, and $\mathcal{F}_{semantic}$ represent visual, spatial, and semantic feature spaces respectively, and $\mathcal{F}_{unified}$ is the unified representation space.

\subsubsection{Information Fusion Strategy}
The multi-modal information fusion employs weighted combination of feature representations:

\begin{equation}
\mathcal{F}_{unified} = \sum_{i=1}^{3} w_i \cdot \phi_i(\mathcal{F}_i)
\end{equation}

where $\phi_i$ represents the transformation function for each modality, and $w_i$ are learned weights that adapt to the relative importance of each information source.

This theoretical framework provides the mathematical foundation for our three-stage architecture, ensuring principled integration of computer vision, knowledge representation, and natural language generation components.

\section{Proposed Approach}

\subsection{Overall Architecture}
Our proposed architecture consists of three main stages: (1) Spatial Feature Extraction, (2) Knowledge Graph Construction, and (3) Depth-Aware Caption Generation. 
Each stage is designed to progressively enrich the input image data with spatial and semantic information, culminating in the generation of detailed and contextually 
relevant captions.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/System1.pdf}
    \caption{Overall Architecture of the Proposed Approach}
    \label{fig:architecture}
\end{figure}

\subsection{Stage 1: Initial Visual-Spatial Knowledge Generation}
\label{sec:stage1}

The first stage of our proposed architecture focuses on generating a preliminary, intrinsic Knowledge Graph (KG) that encodes both the semantic entities and their derived spatial-structural relationships within the input image. This initial graph, denoted as the Internal Knowledge Graph ($\mathbf{KG}_{\text{Int}}$), serves as the foundational, image-grounded structure before external knowledge enrichment.

As illustrated in Figure 1, Stage 1 encompasses the feature extraction, object-depth fusion, and the construction of the $\mathbf{KG}_{\text{Int}}$.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\textwidth]{figures/System1.pdf}
    \caption{Stage 1 architecture: Initial Visual-Spatial Knowledge Generation}
    \label{fig:overall_architecture}
\end{figure}

\subsubsection{Object and Depth Feature Extraction}
We employ a dual-branch feature extraction mechanism.
\begin{enumerate}
    \item \textbf{Object Detection:} The \textbf{YOLOv11x} model is used to detect objects and extract attributes. The output is a set of detections $D = \{d_i\}_{i=1}^n$, where each $d_i$ includes the bounding box $B_i$, class label $C_i$, and confidence score $S_i$.
    \item \textbf{Depth Estimation:} The \textbf{Depth Anything V2} model generates a high-resolution, dense Depth Map $D_{\text{map}} \in \mathbb{R}^{H \times W}$. This map provides the crucial relative depth information necessary for 3D spatial reasoning.
\end{enumerate}

\subsubsection{Object-Depth Fusion and Spatial Logic}
The Fusion and Spatial Logic modules convert the raw visual and depth data into structured relationships.
\begin{enumerate}
    \item \textbf{Quantification and Fusion:} The bounding boxes $B_i$ are aligned with $D_{\text{map}}$. We compute the Average Relative Depth ($\bar{d}_i$) for each object $O_i$ within its bounding box. This step yields a set of Rich Objects $R$, where each object entity is augmented with its spatial center and $\bar{d}_i$. The $\bar{d}_i$ values are used exclusively for internal calculation of relative depth and are not stored as absolute values in the final KG.
    \item \textbf{Spatial Predicate Generation:} The core of this stage is the derivation of new spatial predicates. By comparing the difference in average depth $\mathbf{\Delta d_{i,j}} = \bar{d}_i - \bar{d}_j$ between object pairs $(O_i, O_j)$, we generate a Novel Set of Qualitative 3D Spatial Predicates ($\mathcal{P}_{3D}$):
    $$\mathcal{P}_{3D} \subset \{\text{is\_closer\_than, is\_farther\_than, on\_same\_depth\_plane\_as, in\_immediate\_foreground, etc.}\}$$
    This logic uses dynamic thresholds to convert the continuous $\Delta d_{i,j}$ into discrete, robust relational triplets. Traditional 2D relationships (overlap, adjacency) are computed alongside these 3D predicates (using methods like IoU and Centroid-based Directional logic).
\end{enumerate}

\subsubsection{Internal Scene Graph Construction}
The resulting Rich Objects and the generated 2D/3D relationships are fed into the RDF Knowledge Graph Generator (utilizing libraries such as \texttt{rdflib}). This module performs:
\begin{enumerate}
    \item \textbf{Namespace Definition} and \textbf{Object Node Creation} (from $R$).
    \item \textbf{Relationship Edge Creation} (from $\mathcal{P}_{3D}$ and 2D predicates).
\end{enumerate}
This process finalizes the Internal Knowledge Graph ($\mathbf{KG}_{\text{Int}}$), a set of RDF Triples that accurately models the scene's visual and spatial configuration. This $\mathbf{KG}_{\text{Int}}$ is then serialized (e.g., into a TTL file) as the output of Stage 1, ready for external knowledge enrichment in Stage 2.


\subsection{Stage 2: DASG-CS Construction and Multi-Source Fusion}
\label{sec:stage2}

The primary objective of Stage 2 is the construction of the Depth- and Commonsense-Augmented Scene Graph ($\mathbf{DASG\text{-}CS}$) by integrating the Internal Knowledge Graph ($\mathbf{KG}_{\text{Int}}$) with external knowledge sources. This stage addresses the critical limitation of purely visual KGs by introducing contextual reasoning capabilities. The architecture for this stage is depicted in Figure \ref{fig:stage2_architecture}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/System2.pdf}
    \caption{Architecture of the DASG-CS Construction and Fusion Module (Stage 2)}
    \label{fig:stage2_architecture}
\end{figure}

\subsubsection{Knowledge Source Acquisition}
Three distinct knowledge streams are prepared for fusion:

\begin{enumerate}
    \item \textbf{Internal Visual and 3D Spatial Triples ($\mathbf{KG}_{\text{Int}}$):} This is the output from Stage 1, containing robust 2D semantic triples and our novel $\mathcal{P}_{3D}$ set of qualitative 3D spatial predicates.
    \item \textbf{Factual/Semantic Triples ($\mathbf{KG}_{\text{Fact}}$):} This stream incorporates standard semantic relationships retrieved from large-scale visual knowledge bases (e.g., Visual Genome and Flickr), providing general object-to-object semantic links.
    \item \textbf{Commonsense Triples ($\mathbf{KG}_{\text{CS}}$):} This knowledge is acquired through a two-step process: \textbf{Entity Linking \& Query Generation} uses the object entities (Nodes) from $\mathbf{KG}_{\text{Int}}$ as queries to retrieve relevant triples from the \textbf{ConceptNet} semantic network. This yields $\mathbf{KG}_{\text{CS}}$, which enriches the graph with human-like understanding of context, purpose, and causality.
\end{enumerate}

\subsubsection{Multi-Source Fusion and Graph Consolidation}
The three knowledge streams ($\mathbf{KG}_{\text{Int}}$, $\mathbf{KG}_{\text{Fact}}$, $\mathbf{KG}_{\text{CS}}$) converge into the Multi-Source Fusion Module.

\begin{enumerate}
    \item \textbf{Entity Alignment:} The module first ensures all entities (subjects and objects) are correctly aligned and unified across the three sources.
    \item \textbf{Consolidation:} The module performs a set union operation to form the raw $\mathbf{DASG\text{-}CS}$:
    $$\mathbf{DASG\text{-}CS} = \mathbf{KG}_{\text{Int}} \cup \mathbf{KG}_{\text{Fact}} \cup \mathbf{KG}_{\text{CS}}$$
    \item \textbf{Conflict Resolution:} During consolidation, $\mathbf{KG}_{\text{Int}}$ (the direct visual evidence) is assigned the highest priority to resolve any potential conflicts or inconsistencies arising from general/abstract knowledge found in $\mathbf{KG}_{\text{Fact}}$ or $\mathbf{KG}_{\text{CS}}$.
\end{enumerate}

\subsubsection{Spatial Priority and Commonsense Filtering}
The raw $\mathbf{DASG\text{-}CS}$ is typically redundant and high-dimensional. We introduce a filtering mechanism (represented by the diamond shape in Figure \ref{fig:stage2_architecture}) to retain only the most impactful triples for caption generation:

\begin{enumerate}
    \item \textbf{Spatial Priority Filtering:} Triples containing the Novel 3D Spatial Predicates ($\mathcal{P}_{3D}$) are automatically prioritized and flagged. This ensures the LLM's final output is strongly grounded in the scene's structural awareness.
    \item \textbf{Commonsense Filtering:} $\mathbf{KG}_{\text{CS}}$ triples are filtered based on a computed Relevance Score (e.g., semantic similarity between the triple and the image context) to remove weak or generic commonsense links that might otherwise introduce noise.
\end{enumerate}
This filtering step yields a set of high-quality, non-redundant, and prioritized triples $\mathcal{T}_{\text{opt}}$, ready for the final encoding.

\subsubsection{Structured Sequence Encoding}
The final step of Stage 2 is the Structured Sequence Encoding. The optimized set of triples $\mathcal{T}_{\text{opt}}$ is serialized into a single input sequence $S_{\text{prompt}}$ for the LLM. This encoding uses specific delimiter tokens (e.g., \texttt{[3D\_START]}, \texttt{[CS\_FACT]}) to clearly distinguish the type of knowledge, allowing the LLM in Stage 3 to explicitly learn to weight and utilize each semantic and spatial component effectively.

\subsection{Spatially-Aware LLM Fine-Tuning}

\subsection{Implementation Details and Dataset Preparation}

\section{Evaluation}
\subsection{Environment and Experiment Data}

\section{Future Work}



\section*{Acknowledgments}
The authors would like to thank the Faculty of Information Technology, Ho Chi Minh
 City University of Industry and Trade, and University of Science, VNU-HCM, which
 are sponsors of this research. We also thank anonymous reviewers for their helpful
 comments on this paper.

\bibliographystyle{splncs04}
\bibliography{ref}

\end{document}