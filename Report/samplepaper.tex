% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage{cite}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%

\pagestyle{plain}
\begin{document}
%
\title{Spatio-Structural Image Captioning via LLM Fine-Tuning with Depth-Enhanced Scene Graphs}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Le Vinh Thuan\inst{1} \and Nguyen Minh Khoa\inst{2} \and Nguyen Vinh Thanh\inst{3} \and Nguyen Thi Dinh \inst{4,}}

\authorrunning{L. V. Thuan, et al .}

\institute{%
$^{ \{1,2,3 \}}$Faculty of Information Technology, University of Science, VNU-HCM, \\ Ho Chi Minh City, Vietnam, \\
{$^4$Ho Chi Minh City University of Industry and Trade, Ho Chi Minh City, Vietnam} 
\\[6pt]
$^1$\email{lvthuan23@apcs.fitus.edu.vn}, 
$^2$\email{nmkhoa23@apcs.fitus.edu.vn}, 
$^3$\email{23120012@student.hcmus.edu.vn}, 
$^4$\email{dinhnt@huit.edu.vn}
}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in
150--250 words.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%

\section{Introduction}

The automatic generation of natural language descriptions from images remains one of the most challenging and important tasks in computer vision and natural language processing \cite{vinyals2015show, anderson2018bottom}. This interdisciplinary field, commonly known as image captioning, requires machines to not only perceive objects and their relationships but also to generate coherent, contextually appropriate text that captures the scene's semantic essence \cite{xu2015show, anderson2018bottom}.
Recent advances in deep learning, particularly the adoption of Transformer-based models and Large Language Models (LLMs) in Vision-Language Pre-training (VLP) \cite{li2022blip, alayrac2022flamingo}, have significantly improved descriptive fluency. However, a critical limitation persists: current VLP and image captioning methods primarily rely on 2D feature extraction and lack a deep, grounded understanding of the 3D structure and spatial relationships within a scene. Consequently, generated captions often fail to accurately describe position, depth, and structural context (e.g., "the car is in front of the building" without specifying the distance or depth plane), limiting their applicability in domains requiring precise spatial reasoning, such as robotics or complex scene analysis.
To address this gap, Knowledge Graphs (KGs) offer a crucial pathway by providing structured semantic information and enabling reasoning capabilities \cite{marino2017ok, wu2017image}. While previous work has used KGs to incorporate general semantic and factual knowledge \cite{yao2018exploring, li2022oscar}, few have effectively integrated explicit structural and geometric knowledge like scene depth and 3D spatial relationships. Furthermore, existing KG-enhanced methods often lack commonsense reasoning, which is vital for generating captions that reflect human-like understanding of causality and purpose \cite{zhao2021knowledge}.
This paper introduces a novel framework, the Depth- and Commonsense-Augmented Scene Graph Captioner (DASG-CS Captioner), designed to generate fine-grained image descriptions with enhanced Spatio-Structural and Commonsense-Grounded awareness. We propose a comprehensive pipeline that moves beyond 2D perception by integrating state-of-the-art depth estimation into the knowledge representation stage, thereby enriching the generated captions with precise spatial details. Our method leverages the structured reasoning capabilities of Knowledge Graphs and the generative power of Fine-Tuned LLMs.
The main contributions of this work are:
A Novel Depth- and Commonsense-Augmented Scene Graph (DASG-CS): We propose a new KG architecture that seamlessly fuses visual features, quantitative depth information (from Depth Anything v2), and commonsense knowledge (from ConceptNet) to create a multi-faceted, structured representation of the scene.
Spatially-Aware LLM Fine-Tuning Strategy: We develop an effective encoding and fine-tuning strategy for LLMs, enabling the model to explicitly utilize the Spatio-Structural and Commonsense information encoded in the DASG-CS, leading to generated captions that are demonstrably richer in depth and contextual reasoning.
A Three-Stage Pipeline: We present a pipeline that integrates feature extraction, multi-source knowledge fusion, structured data encoding, and natural language generation, setting a new benchmark for spatially and contextually grounded image captioning.
Extensive Evaluation with Novel Metrics: We conduct a comprehensive evaluation demonstrating the effectiveness of the DASG-CS framework, including specialized metrics to quantify the improvement in Spatial Reasoning Accuracy and Contextual Richness of the generated descriptions.
\section{Related Work}

Image captioning has evolved significantly from simple encoder-decoder architectures \cite{vinyals2015show} to sophisticated systems incorporating spatial awareness and external knowledge \cite{anderson2018bottom}. Our work builds upon three critical research directions: depth-aware image understanding, knowledge graph construction for vision-language tasks, and fine-tuning language models for spatial-aware captioning.

\subsection{Object Detection and Spatial Feature Extraction}

Traditional image captioning systems rely primarily on 2D visual features extracted from RGB images \cite{xu2015show, anderson2018bottom}. The YOLO family has evolved from YOLOv1's single-stage detection \cite{redmon2016you} through multiple generations \cite{bochkovskiy2020yolov4, YOLOv7_ref, YOLO_family_ref} to more sophisticated architectures. Modern object detectors like YOLOv11 provide real-time performance with improved accuracy, particularly for small objects and crowded scenes. While object detection has advanced significantly, most systems still focus on 2D bounding boxes without incorporating depth information for true 3D spatial understanding.

\subsection{Scene Understanding and Spatial Relationships}

Scene graph generation has emerged as a method to capture object relationships explicitly \cite{SceneGraphRef, yao2018exploring}. These approaches construct structured representations of visual scenes by identifying objects and their relationships \cite{zhong2020comprehensive}. However, most scene graphs focus on semantic relationships rather than precise metric spatial relationships. Our approach aims to incorporate depth estimation to enable more accurate spatial relationship modeling.

\subsection{Knowledge-Enhanced Image Captioning}

Knowledge graphs provide structured representations of visual scenes and external world knowledge, enabling richer semantic understanding. Visual Genome \cite{krishna2017visual} pioneered large-scale visual knowledge graphs with detailed annotations of objects, attributes, and relationships. Flickr30k \cite{young2014image} provide image-caption pairs widely used for training vision-language models.

Several works have explored incorporating external knowledge into image captioning. Knowledge graphs have been used for visual question answering \cite{KGforVQARef} and image classification \cite{marino2017ok}. Entity linking techniques \cite{EntityLinkingRef} help map visual detections to knowledge base entities. Knowledge-based approaches \cite{KnowledgeBasedCaptioningRef, zhao2021knowledge} have shown improvements by incorporating common-sense and factual knowledge.

Most existing approaches integrate knowledge at the generation stage \cite{wu2017image}. Our pipeline differs by constructing a comprehensive knowledge graph that fuses visual observations from datasets like Visual Genome and Flickr with external knowledge sources before the generation phase.

\subsection{Natural Language Generation for Image Captioning}

Natural language generation is the final component of image captioning systems. Early encoder-decoder models \cite{vinyals2015show, karpathy2015deep} used RNNs to generate captions from visual features. Attention mechanisms \cite{xu2015show, anderson2018bottom} improved performance by allowing models to focus on relevant image regions during generation.

More recent approaches employ transformer-based architectures \cite{li2019entangled, cornia2020meshed} for better long-range dependency modeling. The T5 model \cite{raffel2020exploring} treats all NLP tasks as text-to-text problems, offering a unified framework adaptable to various generation tasks including image captioning.

Our approach leverages text-to-text transformers for generating descriptions from structured semantic representations. By fine-tuning on data that explicitly encodes spatial relationships and depth information, we aim to generate descriptions that accurately reflect 3D spatial configurations.

\subsection{Research Gaps and Our Contributions}

Existing image captioning systems face several limitations:

\begin{enumerate}
\item \textbf{Limited Spatial Awareness:} Most systems \cite{vinyals2015show, xu2015show} rely on 2D visual features without depth information for true 3D spatial understanding.

\item \textbf{Knowledge Integration:} While some works integrate external knowledge \cite{marino2017ok, zhao2021knowledge}, comprehensive frameworks that systematically combine visual observations with external knowledge graphs remain limited.

\item \textbf{Spatial Description Quality:} Generated descriptions often lack precise spatial relationships and depth-aware positioning information.
\end{enumerate}

Our proposed three-system pipeline addresses these gaps:

\begin{enumerate}
\item \textbf{System 1 - Spatial Feature Extraction:} We combine YOLOv11 for object detection with Depth Anything V2 for monocular depth estimation, enabling extraction of objects, their relationships, and depth information.

\item \textbf{System 2 - Knowledge Graph Framework:} We construct knowledge graphs based on Visual Genome \cite{krishna2017visual} and Flickr \cite{young2014image} datasets, enriched with external knowledge from ConceptNet and DBpedia.

\item \textbf{System 3 - Depth-Aware Caption Generation:} We fine-tune language models to generate image descriptions with emphasis on spatial and depth features, producing captions that accurately reflect 3D spatial configurations.
\end{enumerate}

The key contribution is the systematic integration of depth information throughout the pipeline, from detection through knowledge graph construction to caption generation, emphasizing spatial accuracy in generated descriptions.



\section{Theoretical Basis}

This section presents the theoretical foundations underlying our three-stage architecture, providing formal definitions and mathematical models that support the integration of computer vision, knowledge graph reasoning, and natural language generation.

\subsection{Object Detection and Spatial Analysis Framework}

\subsubsection{YOLOv11 Detection Model}
The YOLOv11 object detection framework can be formalized as a function $f_{YOLO}: \mathbb{R}^{H \times W \times 3} \rightarrow \mathcal{D}$, where the input image $I \in \mathbb{R}^{H \times W \times 3}$ is mapped to a detection set $\mathcal{D} = \{d_1, d_2, ..., d_n\}$. Each detection $d_i$ is represented as:

\begin{equation}
d_i = (bbox_i, c_i, s_i)
\end{equation}

where $bbox_i = (x_i, y_i, w_i, h_i)$ represents the bounding box coordinates, $c_i \in \mathcal{C}$ is the class label from the predefined class set $\mathcal{C}$, and $s_i \in [0,1]$ is the confidence score.

\subsubsection{Depth Estimation and Spatial Reasoning}
We incorporate the Depth-Anything-V2 model \cite{yang2024depth} to estimate depth information, formally defined as:

\begin{equation}
f_{depth}: \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H \times W}
\end{equation}

The depth map $D = f_{depth}(I)$ provides spatial context that enhances object relationship understanding. Combined with the Shapely geometry library \cite{gillies2007shapely}, we compute spatial relationships between detected objects using geometric operations:

\begin{equation}
R_{spatial}(d_i, d_j) = \{distance(d_i, d_j), overlap(d_i, d_j), relative\_position(d_i, d_j)\}
\end{equation}

\subsection{Knowledge Graph Construction Theory}

\subsubsection{Entity Linking and Knowledge Fusion}
Given the detection set $\mathcal{D}$ from Stage I, we define the entity linking function as:

\begin{equation}
f_{link}: \mathcal{D} \times \mathcal{KB} \rightarrow \mathcal{E}_{linked}
\end{equation}

where $\mathcal{KB}$ represents the external knowledge base and $\mathcal{E}_{linked}$ is the set of linked entities. The linking process employs semantic similarity scoring:

\begin{equation}
sim(d_i, e_k) = \alpha \cdot sim_{text}(c_i, label(e_k)) + \beta \cdot sim_{context}(context(d_i), context(e_k))
\end{equation}

where $\alpha$ and $\beta$ are weighting parameters, and $sim_{text}$ and $sim_{context}$ represent textual and contextual similarity measures respectively.

\subsubsection{Knowledge Graph Formalization}
The constructed knowledge graph is formally defined as a directed graph $KG = (V, E, R, A)$ where $V = \{v_1, v_2, ..., v_m\}$ is the set of entity vertices, $E \subseteq V \times V$ represents the edges between entities, $R$ is the set of relation types, and $A: V \rightarrow \mathcal{P}(\mathcal{A})$ maps entities to their attribute sets.

\subsection{T5-based Natural Language Generation Framework}

\subsubsection{Text-to-Text Transfer Learning}
Our approach leverages the T5 (Text-to-Text Transfer Transformer) model \cite{raffel2020exploring} for generating natural language descriptions from structured semantic representations. The T5 framework treats all NLP tasks as text-to-text problems:

\begin{equation}
f_{T5}: \mathcal{S}_{semantic} \rightarrow \mathcal{T}_{text}
\end{equation}

where $\mathcal{S}_{semantic}$ represents the semantic input derived from the knowledge graph and $\mathcal{T}_{text}$ is the generated natural language output.

\subsubsection{Semantic Representation Encoding}
The knowledge graph entities and relationships are encoded into a structured semantic representation $S_{semantic}$ that serves as input to the T5 model:

\begin{equation}
S_{semantic} = encode(KG, R_{spatial}, C_{context})
\end{equation}

where $C_{context}$ represents the contextual information derived from the enriched data analysis.

\subsubsection{Attention Mechanism for Semantic Focus}
The T5 model employs multi-head self-attention to focus on relevant semantic elements:

\begin{equation}
Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $Q$, $K$, and $V$ represent query, key, and value matrices respectively, derived from the semantic representation.

\subsection{Contextual Reasoning and Inference Theory}

\subsubsection{Enriched Data Analysis}
Our approach performs contextual reasoning using enriched data from the knowledge graph without explicit relationship modeling. The contextual inference function is defined as:

\begin{equation}
f_{inference}: \mathcal{E}_{enriched} \rightarrow \mathcal{I}_{context}
\end{equation}

where $\mathcal{E}_{enriched}$ represents the enriched entity set and $\mathcal{I}_{context}$ denotes the inferred contextual information.

\subsubsection{Semantic Coherence Optimization}
To ensure semantic coherence in the generated descriptions, we optimize the following objective function:

\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{generation} + \lambda_1 \mathcal{L}_{coherence} + \lambda_2 \mathcal{L}_{semantic}
\end{equation}

where $\mathcal{L}_{generation}$ is the standard language modeling loss, $\mathcal{L}_{coherence}$ measures textual coherence, $\mathcal{L}_{semantic}$ ensures semantic consistency with the input knowledge graph, and $\lambda_1$, $\lambda_2$ are regularization parameters.

\subsection{Multi-Modal Integration Theory}

\subsubsection{Cross-Modal Alignment}
The integration of visual features, spatial information, and semantic knowledge requires cross-modal alignment. We define the alignment function as:

\begin{equation}
f_{align}: \mathcal{F}_{visual} \times \mathcal{F}_{spatial} \times \mathcal{F}_{semantic} \rightarrow \mathcal{F}_{unified}
\end{equation}

where $\mathcal{F}_{visual}$, $\mathcal{F}_{spatial}$, and $\mathcal{F}_{semantic}$ represent visual, spatial, and semantic feature spaces respectively, and $\mathcal{F}_{unified}$ is the unified representation space.

\subsubsection{Information Fusion Strategy}
The multi-modal information fusion employs weighted combination of feature representations:

\begin{equation}
\mathcal{F}_{unified} = \sum_{i=1}^{3} w_i \cdot \phi_i(\mathcal{F}_i)
\end{equation}

where $\phi_i$ represents the transformation function for each modality, and $w_i$ are learned weights that adapt to the relative importance of each information source.

This theoretical framework provides the mathematical foundation for our three-stage architecture, ensuring principled integration of computer vision, knowledge representation, and natural language generation components.

\section{Proposed Approach}

\subsection{Overall Architecture}
Our proposed architecture consists of three main stages: (1) Spatial Feature Extraction, (2) Knowledge Graph Construction, and (3) Depth-Aware Caption Generation. 
Each stage is designed to progressively enrich the input image data with spatial and semantic information, culminating in the generation of detailed and contextually 
relevant captions.

\subsection{Entities extraction}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/System1.pdf}
    \caption{Overall Architecture of the Proposed Approach}
    \label{fig:architecture}
\end{figure}

\section{Evaluation}
\subsection{Environment and Experiment Data}

\section{Future Work}



\section*{Acknowledgments}
The authors would like to thank the Faculty of Information Technology, Ho Chi Minh
 City University of Industry and Trade, and University of Science, VNU-HCM, which
 are sponsors of this research. We also thank anonymous reviewers for their helpful
 comments on this paper.

\bibliographystyle{splncs04}
\bibliography{ref}

\end{document}